{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "E1wrUxztQxPl"
      },
      "source": [
        "# Imports"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "t93PFznBO0bZ"
      },
      "source": [
        "from __future__ import absolute_import, division, print_function, unicode_literals\n",
        "\n",
        "try:\n",
        "  # %tensorflow_version only exists in Colab.\n",
        "  %tensorflow_version 2.x\n",
        "except Exception:\n",
        "  pass\n",
        "import tensorflow as tf\n",
        "\n",
        "import numpy as np\n",
        "\n",
        "import matplotlib as mpl\n",
        "\n",
        "from IPython.display import clear_output\n",
        "from matplotlib import pyplot as plt\n",
        "from tensorflow.keras.preprocessing import image"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7v_OJAN25Rl_"
      },
      "source": [
        "# Load Images"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gVTFtYlt5UVB"
      },
      "source": [
        "# Load images of choice"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ICgN-EskRmo8"
      },
      "source": [
        "\n",
        "#Optional If you want to load images of your choice\n",
        "from google.colab import files\n",
        "files.upload()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "s6VTCRuO49sa"
      },
      "source": [
        "image_path = # path to current image (you can check it using os.listdir())\n",
        "def download(image_path, target_size=None):\n",
        "  img = tf.keras.preprocessing.image.load_img(image_path, target_size=target_size)\n",
        "  return img\n",
        "\n",
        "original_img = download(image_path, target_size=[225, 375])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LJxZwDGf5YXa"
      },
      "source": [
        "## Load official tutorial images"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Gtkhq5JZRA-X"
      },
      "source": [
        "# Load image from url\n",
        "url = 'https://storage.googleapis.com/download.tensorflow.org/example_images/YellowLabradorLooking_new.jpg'\n",
        "\n",
        "def download(url, target_size=None):\n",
        "  name = url.split('/')[-1]\n",
        "  image_path = tf.keras.utils.get_file(name, origin=url)\n",
        "  img = tf.keras.preprocessing.image.load_img(image_path, target_size=target_size)\n",
        "  return img\n",
        "\n",
        "original_img = download(url, target_size=[225, 375])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2l4iI-uB5d3t"
      },
      "source": [
        "## Plot selected image"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "W252H7FzRt3J"
      },
      "source": [
        "\n",
        "# Normalize an image\n",
        "def deprocess(img):\n",
        "  img = 255*(img + 1.0)/2.0\n",
        "  return tf.cast(img, tf.uint8)\n",
        "\n",
        "\n",
        "# Display an image\n",
        "def show(img):\n",
        "  plt.figure(figsize=(12,12))\n",
        "  plt.grid(False)\n",
        "  plt.axis('off')\n",
        "  plt.imshow(img)\n",
        "  plt.show()\n",
        "\n",
        "url = 'giant-tortoise.jpg'\n",
        "# Downsizing the image makes it easier to work with.\n",
        "original_img = np.array(original_img)\n",
        "\n",
        "show(original_img)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jOq4DhdJSuuT"
      },
      "source": [
        "# Prepare the feature extraction model: FILL THE CODE\n",
        "\n",
        "we will load and prepare a pre-trained image classification model. In this case \n",
        "we are going to use InceptionV3, which is similar to the model originally used in DeepDream. *N.B. any pre-trained model will work, however you will have to adjust the layer names*"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4d6yTw0ETE88"
      },
      "source": [
        "base_model = tf.keras.applications.InceptionV3(include_top=False, weights='imagenet')\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ofw4kcrITi_m"
      },
      "source": [
        "DeepDream main idea:\n",
        "\n",
        "\n",
        "*   choose one or more layers and maximize the loss to increasingly \"excite\" the layers\n",
        "*   Complexity of features depends on the chosen layers (lower layers correspond to simpler features, while higher layers to more sophisticated ones)\n",
        "\n",
        "InceptionV3\n",
        "\n",
        "\n",
        "\n",
        "*   Large architecture\n",
        "*   Layers of interest are the ones where the convolutions are concatenated.\n",
        "*   Whatever layer can be chosen, however deeper ones will need more time to train\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "S-PbSISAUYdE"
      },
      "source": [
        "# Maximize the activations of these layers\n",
        "names = ['mixed3', 'mixed5']\n",
        "\n",
        "# FILL THE CODE: Extract the layers contained in \"layers\" from \"base_model\"\n",
        "layers = [base_model.get_layer(name).output for name in names]\n",
        "\n",
        "# FILL THE CODE: Create the feature extraction model using as input \n",
        "# based_model.input  and as output the output of the selected layers\n",
        "dream_model = tf.keras.Model(inputs=base_model.input, outputs=layers)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wJk7TiM2UgL5"
      },
      "source": [
        "# Calculate the loss\n",
        "\n",
        "loss is given by \n",
        "\n",
        "\n",
        "*   Sum of activations in the chosen layers\n",
        "*   Is normalized at each layer, so that contribution from bigger layers does not outweigh contributions from smaller ones\n",
        "\n",
        "N.B. normally a deep learning model is trained using Gradient descent, in this case we will use Gradient Ascent, since we want to maximize the loss\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WC-C4zkkVrxe"
      },
      "source": [
        "def calc_loss(img, model):\n",
        "  # Pass forward the image through the model to retrieve the activations.\n",
        "  # Converts the image into a batch of size 1.\n",
        "  img_batch = tf.expand_dims(img, axis=0)\n",
        "  layer_activations = model(img_batch)\n",
        "\n",
        "  losses = []\n",
        "  for act in layer_activations:\n",
        "    loss = tf.math.reduce_mean(act)\n",
        "    losses.append(loss)\n",
        "\n",
        "  return  tf.reduce_sum(losses)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZcoWPtugWl2e"
      },
      "source": [
        "# Gradient Ascent\n",
        "\n",
        "Once you have calculated losses for the chosen layers:\n",
        "*   Calculate gradients w.r.t. the image\n",
        "*   Add them to the original image\n",
        "In this way we enhance patterns seen by the network, at each step we increasingly excite the activations of certain layers in the network.\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uwOVOn8zXF0M"
      },
      "source": [
        "@tf.function\n",
        "def deepdream(model, img, step_size):\n",
        "    with tf.GradientTape() as tape:\n",
        "      # This needs gradients relative to `img`\n",
        "      # `GradientTape` only watches `tf.Variable`s by default\n",
        "      tape.watch(img)\n",
        "      loss = calc_loss(img, model)\n",
        "\n",
        "    # FILL THE CODE: Calculate the gradient of the loss with respect to the pixels of the input image.\n",
        "    gradients = tape.gradient(loss, img)\n",
        "\n",
        "    # Normalize the gradients.\n",
        "    gradients /= tf.math.reduce_std(gradients) + 1e-8 \n",
        "    \n",
        "    # FILL THE CODE: In gradient ascent, the \"loss\" is maximized so that the input image increasingly \n",
        "    # \"excites\" the layers.\n",
        "    # You can update the image by directly adding the gradients (because they're the same shape!) \n",
        "    # to the image\n",
        "    img = img + gradients*step_size\n",
        "    img = tf.clip_by_value(img, -1, 1)\n",
        "\n",
        "    return loss, img"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QRj4Eg1AX6-q"
      },
      "source": [
        "def run_deep_dream_simple(model, img, steps=100, step_size=0.01):\n",
        "  # Convert from uint8 to the range expected by the model.\n",
        "  img = tf.keras.applications.inception_v3.preprocess_input(img)\n",
        "\n",
        "  for step in range(steps):\n",
        "    loss, img = deepdream(model, img, step_size)\n",
        "    \n",
        "    if step % 100 == 0:\n",
        "      clear_output(wait=True)\n",
        "      show(deprocess(img))\n",
        "      print (\"Step {}, loss {}\".format(step, loss))\n",
        "\n",
        "\n",
        "  result = deprocess(img)\n",
        "  clear_output(wait=True)\n",
        "  show(result)\n",
        "\n",
        "  return result\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-7owEGQCYd1N"
      },
      "source": [
        "dream_img = run_deep_dream_simple(model=dream_model, img=original_img, steps=800, step_size=0.001)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tvdGnm1VakaF"
      },
      "source": [
        "# Scale up with octave and tiles\n",
        "\n",
        "There are a few problems with this basic implementation\n",
        "\n",
        "\n",
        "*   Output is noisy (same as in style transfer)\n",
        "*   The image is low resolution.\n",
        "*   Patterns appear like they're all happening at the same granularity.\n",
        "\n",
        "One approach that solves all of these issues is to run Gradient Ascent at different scales. This allows patterns generated at smaller scales to be incorporated into patterns at higher scales and filled in with additional detail.\n",
        "\n",
        "This can be done by repeating the gradient descent procedure, each time upscaling the image. \n",
        "However this approach does not **scale** well with the size of the image.\n",
        "\n",
        "Therefore we do the following:\n",
        "\n",
        "# Splitting up with tiles\n",
        "\n",
        "Images are splitted into tiles and then gradient is computed for each tile, applying random shifts before image generation, prevents tile seams from appearing."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "132Zvr6qcv31"
      },
      "source": [
        "def random_roll(img, maxroll):\n",
        "  # Randomly shift the image to avoid tiled boundaries.\n",
        "  shift = tf.random.uniform(shape=[2], minval=-maxroll, maxval=maxroll, dtype=tf.int32)\n",
        "  shift_down, shift_right = shift[0],shift[1] \n",
        "\n",
        "  # FILL THE CODE: shift image first along right and then along down directions\n",
        "  # HINT: use tf.roll\n",
        "  img_rolled = tf.roll(tf.roll(img, shift_right, axis=1), shift_down, axis=0)\n",
        "  return shift_down, shift_right, img_rolled\n",
        "\n",
        "# Apply random roll and show result\n",
        "shift_down, shift_right, img_rolled = random_roll(np.array(original_img), 512)\n",
        "show(img_rolled)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "P8NvEK1Lc3X0"
      },
      "source": [
        "Then we can define a tiled equivalent of the previously defined deepdream function"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "75JJT57rc8rn"
      },
      "source": [
        "@tf.function\n",
        "\n",
        "def get_tiled_gradients(model, img, tile_size = 512):\n",
        "  shift_down, shift_right, img_rolled = random_roll(img, tile_size)\n",
        "\n",
        "  # Initialize the image gradients to zero.\n",
        "  gradients = tf.zeros_like(img_rolled)\n",
        "\n",
        "  for x in tf.range(0, img_rolled.shape[0], tile_size):\n",
        "    for y in tf.range(0, img_rolled.shape[1], tile_size):\n",
        "      # Calculate the gradients for this tile.\n",
        "      with tf.GradientTape() as tape:\n",
        "        # This needs gradients relative to `img_rolled`.\n",
        "        # `GradientTape` only watches `tf.Variable`s by default.\n",
        "        tape.watch(img_rolled)\n",
        "\n",
        "        # Extract a tile out of the image.\n",
        "        img_tile = img_rolled[x:x+tile_size, y:y+tile_size]\n",
        "        loss = calc_loss(img_tile, model)\n",
        "\n",
        "      # Update the image gradients for this tile.\n",
        "      gradients = gradients + tape.gradient(loss, img_rolled)\n",
        "\n",
        "  # Undo the random shift applied to the image and its gradients.\n",
        "  gradients = tf.roll(tf.roll(gradients, -shift_right, axis=1), -shift_down, axis=0)\n",
        "\n",
        "  # Normalize the gradients.\n",
        "  gradients /= tf.math.reduce_std(gradients) + 1e-8 \n",
        "\n",
        "  return gradients "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "I50Jhs_qdT9X"
      },
      "source": [
        "def run_deep_dream_with_octaves(model, img, steps_per_octave=100, step_size=0.01, \n",
        "                                num_octaves=3, octave_scale=1.3):\n",
        "  img = tf.keras.preprocessing.image.img_to_array(img)\n",
        "  img = tf.keras.applications.inception_v3.preprocess_input(img)\n",
        "\n",
        "  for octave in range(num_octaves):\n",
        "    # Scale the image based on the octave\n",
        "    if octave>0:\n",
        "      new_size = tf.cast(tf.convert_to_tensor(img.shape[:2]), tf.float32)*octave_scale\n",
        "      img = tf.image.resize(img, tf.cast(new_size, tf.int32))\n",
        "\n",
        "    for step in range(steps_per_octave):\n",
        "      gradients = get_tiled_gradients(model, img)\n",
        "      img = img + gradients*step_size\n",
        "      img = tf.clip_by_value(img, -1, 1)\n",
        "\n",
        "      if step % 10 == 0:\n",
        "        clear_output(wait=True)\n",
        "        show(deprocess(img))\n",
        "        print (\"Octave {}, Step {}\".format(octave, step))\n",
        "    \n",
        "  clear_output(wait=True)\n",
        "  result = deprocess(img)\n",
        "  show(result)\n",
        "\n",
        "  return result"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3DikvB5XdZUb",
        "collapsed": true
      },
      "source": [
        "dream_img = run_deep_dream_with_octaves(model=dream_model, img=original_img, step_size=0.01)\n",
        "\n",
        "clear_output()\n",
        "show(original_img)\n",
        "show(dream_img)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8LGEQhuE99xG"
      },
      "source": [],
      "execution_count": null,
      "outputs": []
    }
  ]
}