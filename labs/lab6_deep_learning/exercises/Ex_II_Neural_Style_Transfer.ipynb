{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "name": "Ex_II_Neural_Style_Transfer.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Tgwqw_Y3Zeel"
      },
      "source": [
        "# Imports"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "q4edmzviZeen"
      },
      "source": [
        "from __future__ import absolute_import, division, print_function, unicode_literals\n",
        "\n",
        "try:\n",
        "  # %tensorflow_version only exists in Colab.\n",
        "  %tensorflow_version 2.x\n",
        "except Exception:\n",
        "  pass\n",
        "import tensorflow as tf\n",
        "\n",
        "import IPython.display as display\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "import matplotlib as mpl\n",
        "mpl.rcParams['figure.figsize'] = (12,12)\n",
        "mpl.rcParams['axes.grid'] = False\n",
        "\n",
        "import numpy as np\n",
        "import PIL.Image\n",
        "import time\n",
        "import functools"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NZsXZAAxZeeq"
      },
      "source": [
        "# Functions"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Tf_ivVbCZeeq"
      },
      "source": [
        "# Converts tensor into image\n",
        "def tensor_to_image(tensor):\n",
        "    tensor = tensor*255\n",
        "    tensor = np.array(tensor, dtype=np.uint8)\n",
        "    if np.ndim(tensor)>3:\n",
        "        assert tensor.shape[0] == 1\n",
        "        tensor = tensor[0]\n",
        "    return PIL.Image.fromarray(tensor)\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MQe8zUGUZees"
      },
      "source": [
        "# Load Images"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "q-PIOdLX4b6o"
      },
      "source": [
        "## Load images of choice"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Tq0cx7t3i40Z"
      },
      "source": [
        "#Optional If you want to load images of your choice\n",
        "from google.colab import files\n",
        "files.upload()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2ubYOwUz4gAM"
      },
      "source": [
        "# Optional\n",
        "# Change to local path\n",
        "style_path = #...\n",
        "# Change to local path\n",
        "content_path = #...\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZOF46K0D4m1P"
      },
      "source": [
        "# Load official tutorial images"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KRT9xkKBZeet"
      },
      "source": [
        "content_path = tf.keras.utils.get_file('YellowLabradorLooking_new.jpg', 'https://storage.googleapis.com/download.tensorflow.org/example_images/YellowLabradorLooking_new.jpg')\n",
        "\n",
        "# https://commons.wikimedia.org/wiki/File:Vassily_Kandinsky,_1913_-_Composition_7.jpg\n",
        "style_path = tf.keras.utils.get_file('kandinsky5.jpg','https://storage.googleapis.com/download.tensorflow.org/example_images/Vassily_Kandinsky%2C_1913_-_Composition_7.jpg')\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uZsOx7Y_Zeev"
      },
      "source": [
        "# Visualize the input\n",
        "\n",
        "simple function to load an image and limit its maximum dimension to 512 pixels"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sLxPuaokZeev"
      },
      "source": [
        "def load_img(path_to_img):\n",
        "    max_dim = 512\n",
        "    img = tf.io.read_file(path_to_img)\n",
        "    img = tf.image.decode_image(img, channels=3)\n",
        "    img = tf.image.convert_image_dtype(img, tf.float32)\n",
        "\n",
        "    shape = tf.cast(tf.shape(img)[:-1], tf.float32)\n",
        "    long_dim = max(shape)\n",
        "    scale = max_dim / long_dim\n",
        "\n",
        "    new_shape = tf.cast(shape * scale, tf.int32)\n",
        "\n",
        "    img = tf.image.resize(img, new_shape)\n",
        "    img = img[tf.newaxis, :]\n",
        "    return img"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ueraznuTZeex"
      },
      "source": [
        "function to display the image"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "c0H6wrPeZeey"
      },
      "source": [
        "def imshow(image, title=None):\n",
        "    if len(image.shape) > 3:\n",
        "        image = tf.squeeze(image, axis=0)\n",
        "\n",
        "        plt.imshow(image)\n",
        "    if title:\n",
        "        plt.title(title)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "I5WpOXbGZee0"
      },
      "source": [
        "Load the images"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Knfe02PKZee0"
      },
      "source": [
        "content_image = load_img(content_path)\n",
        "style_image = load_img(style_path)\n",
        "\n",
        "plt.subplot(1, 2, 1)\n",
        "imshow(content_image, 'Content Image')\n",
        "\n",
        "plt.subplot(1, 2, 2)\n",
        "imshow(style_image, 'Style Image')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GxkSrlFqZee5"
      },
      "source": [
        "# Define content and style representations"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3Y3_Ejv7Zee6"
      },
      "source": [
        "Use the intermediate layers of the model to get the content and style representations of the image. Starting from the network's input layer, the first few layer activations represent low-level features like edges and textures. As you step through the network, the final few layers represent higher-level featuresâ€”object parts like wheels or eyes. In this case, you are using the VGG19 network architecture, a pretrained image classification network. These intermediate layers are necessary to define the representation of content and style from the images. For an input image, try to match the corresponding style and content target representations at these intermediate layers."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vS2Hv4EoZee7"
      },
      "source": [
        "# Load a VGG19 and test run it on our image to ensure it's used correctly:\n",
        "\n",
        "x = tf.keras.applications.vgg19.preprocess_input(content_image*255)\n",
        "x = tf.image.resize(x, (224, 224))\n",
        "vgg = tf.keras.applications.VGG19(include_top=True, weights='imagenet')\n",
        "prediction_probabilities = vgg(x)\n",
        "prediction_probabilities.shape"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Uho1lGQwZee8"
      },
      "source": [
        "# Check Predictions for current image\n",
        "predicted_top_5 = tf.keras.applications.vgg19.decode_predictions(prediction_probabilities.numpy())[0]\n",
        "[(class_name, prob) for (number, class_name, prob) in predicted_top_5]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fPsu3ZAnZee-"
      },
      "source": [
        "Now load a VGG19 without the classification head, and list the layer names\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mwJ_y96MZee_"
      },
      "source": [
        "vgg = tf.keras.applications.VGG19(include_top=False, weights='imagenet')\n",
        "\n",
        "print()\n",
        "for layer in vgg.layers:\n",
        "    print(layer.name)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EClQ0w3GZefB"
      },
      "source": [
        "Choose intermediate layers from the network to represent the style and content of the image:\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sEoh0UjOZefB"
      },
      "source": [
        "# Content layer where will pull our feature maps\n",
        "content_layers = ['block5_conv2'] \n",
        "\n",
        "# Style layer of interest\n",
        "style_layers = ['block1_conv1',\n",
        "                'block2_conv1',\n",
        "                'block3_conv1', \n",
        "                'block4_conv1', \n",
        "                'block5_conv1']\n",
        "\n",
        "num_content_layers = len(content_layers)\n",
        "num_style_layers = len(style_layers)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mIGUiYzrZefD"
      },
      "source": [
        "Intermediate layers for style and content\n",
        "So why do these intermediate outputs within our pretrained image classification network allow us to define style and content representations?\n",
        "\n",
        "At a high level, in order for a network to perform image classification (which this network has been trained to do), it must understand the image. This requires taking the raw image as input pixels and building an internal representation that converts the raw image pixels into a complex understanding of the features present within the image.\n",
        "\n",
        "This is also a reason why convolutional neural networks are able to generalize well: theyâ€™re able to capture the invariances and defining features within classes (e.g. cats vs. dogs) that are agnostic to background noise and other nuisances. Thus, somewhere between where the raw image is fed into the model and the output classification label, the model serves as a complex feature extractor. By accessing intermediate layers of the model, you're able to describe the content and style of input images.\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CC-YtG0lZefF"
      },
      "source": [
        "# Build the model: FILL THE CODE"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "96UYgCy0ZefG"
      },
      "source": [
        "The networks in tf.keras.applications are designed so you can easily extract the intermediate layer values using the Keras functional API.\n",
        "\n",
        "To define a model using the functional API, specify the inputs and outputs:\n",
        "\n",
        "model = Model(inputs, outputs)\n",
        "\n",
        "This following function builds a VGG19 model that returns a list of intermediate layer outputs:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vtd2IdvLZefG"
      },
      "source": [
        "def vgg_layers(layer_names):\n",
        "    \"\"\" Creates a vgg model that returns a list of intermediate output values.\"\"\"\n",
        "    # Load our model. Load pretrained VGG, trained on imagenet data\n",
        "    vgg = tf.keras.applications.VGG19(include_top=False, weights='imagenet')\n",
        "\n",
        "    # We want to use it as an extractor, so set to false train option\n",
        "    vgg.trainable = False\n",
        "\n",
        "    # FILL THE CODE: outputs must be a list containing tensor with the outputs \n",
        "    # of each layer contained in layer_names\n",
        "    # HINT: from a functional model the outputs can be extracted using \n",
        "    # model.get_layer(name).output\n",
        "    outputs = # ...\n",
        "\n",
        "\n",
        "    # FILL THE CODE Build a functional model that gets vgg.input as input and  \n",
        "    # outputs the content of each layer contained in layer_names\n",
        "    model = # ...\n",
        "    return model"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2yjFOu9dZefI"
      },
      "source": [
        "create model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Zab3mxW_ZefI"
      },
      "source": [
        "style_extractor = vgg_layers(style_layers)\n",
        "style_outputs = style_extractor(style_image*255)\n",
        "\n",
        "#Look at the statistics of each layer's output\n",
        "for name, output in zip(style_layers, style_outputs):\n",
        "    print(name)\n",
        "    print(\"  shape: \", output.numpy().shape)\n",
        "    print(\"  min: \", output.numpy().min())\n",
        "    print(\"  max: \", output.numpy().max())\n",
        "    print(\"  mean: \", output.numpy().mean())\n",
        "    print()\n",
        "    \n",
        "# Model summary\n",
        "style_extractor.summary()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "W1cLsxoMZefK"
      },
      "source": [
        "# Calculate style"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GLS9sTzQZefL"
      },
      "source": [
        "The content of an image is represented by the values of the intermediate feature maps.\n",
        "\n",
        "It turns out, the style of an image can be described by the means and correlations across the different feature maps. Calculate a Gram matrix that includes this information by taking the outer product of the feature vector with itself at each location, and averaging that outer product over all locations. \n",
        "\n",
        "$G_{cd}^l  = \\frac{\\sum_{ij}F_{ijc}^l(x)F_{ijd}^l(x)}{IJ}$\n",
        "\n",
        "where $l$ is the layer index, $i$ and $j$ are the pixel indices and $c$ $d$ represent two different filters, $x$ is a general input to the layer "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Y_MQURqCZefL"
      },
      "source": [
        "def gram_matrix(input_tensor):\n",
        "    result = tf.linalg.einsum('bijc,bijd->bcd', input_tensor, input_tensor) # Check\n",
        "    input_shape = tf.shape(input_tensor)\n",
        "    num_locations = tf.cast(input_shape[1]*input_shape[2], tf.float32)\n",
        "    return result/(num_locations)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HziOH1YSZefN"
      },
      "source": [
        "# Extract syle and content"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MIiRB6YCZefO"
      },
      "source": [
        "class StyleContentModel(tf.keras.models.Model):\n",
        "    def __init__(self, style_layers, content_layers):\n",
        "        super(StyleContentModel, self).__init__()\n",
        "        self.vgg =  vgg_layers(style_layers + content_layers)\n",
        "        self.style_layers = style_layers\n",
        "        self.content_layers = content_layers\n",
        "        self.num_style_layers = len(style_layers)\n",
        "        self.vgg.trainable = False\n",
        "\n",
        "    def call(self, inputs):\n",
        "        \"Expects float input in [0,1]\"\n",
        "        inputs = inputs*255.0\n",
        "        preprocessed_input = tf.keras.applications.vgg19.preprocess_input(inputs)\n",
        "        outputs = self.vgg(preprocessed_input)\n",
        "        style_outputs, content_outputs = (outputs[:self.num_style_layers], \n",
        "                                          outputs[self.num_style_layers:])\n",
        "\n",
        "        style_outputs = [gram_matrix(style_output)\n",
        "                         for style_output in style_outputs]\n",
        "\n",
        "        content_dict = {content_name:value \n",
        "                        for content_name, value \n",
        "                        in zip(self.content_layers, content_outputs)}\n",
        "\n",
        "        style_dict = {style_name:value\n",
        "                      for style_name, value\n",
        "                      in zip(self.style_layers, style_outputs)}\n",
        "\n",
        "        return {'content':content_dict, 'style':style_dict}"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NYEwcStJZefQ"
      },
      "source": [
        "extractor = StyleContentModel(style_layers, content_layers)\n",
        "\n",
        "results = extractor(tf.constant(content_image))\n",
        "\n",
        "style_results = results['style']\n",
        "\n",
        "print('Styles:')\n",
        "for name, output in sorted(results['style'].items()):\n",
        "    print(\"  \", name)\n",
        "    print(\"    shape: \", output.numpy().shape)\n",
        "    print(\"    min: \", output.numpy().min())\n",
        "    print(\"    max: \", output.numpy().max())\n",
        "    print(\"    mean: \", output.numpy().mean())\n",
        "    print()\n",
        "\n",
        "    print(\"Contents:\")\n",
        "for name, output in sorted(results['content'].items()):\n",
        "    print(\"  \", name)\n",
        "    print(\"    shape: \", output.numpy().shape)\n",
        "    print(\"    min: \", output.numpy().min())\n",
        "    print(\"    max: \", output.numpy().max())\n",
        "    print(\"    mean: \", output.numpy().mean())"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zY7eB3EHZefT"
      },
      "source": [
        "# Run gradient descent: FILL THE CODE"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xW6a2jXUZefU"
      },
      "source": [
        "# Set your style and content target values:\n",
        "style_targets = extractor(style_image)['style']\n",
        "content_targets = extractor(content_image)['content']\n",
        "\n",
        "# Define a tf.Variable to contain the image to optimize. To make this quick, initialize it with the content image \n",
        "image = tf.Variable(content_image)\n",
        "\n",
        "# Clip the image\n",
        "def clip_0_1(image):\n",
        "    return tf.clip_by_value(image, clip_value_min=0.0, clip_value_max=1.0)\n",
        "\n",
        "# Optimizer \n",
        "opt = tf.keras.optimizers.Adam(learning_rate=0.02, beta_1=0.99, epsilon=1e-1)\n",
        "\n",
        "style_weight=1e-2\n",
        "content_weight=1e4\n",
        "\n",
        "# To optimize this, use a weighted combination of the two losses to get the total loss:\n",
        "def style_content_loss(outputs):\n",
        "    style_outputs = outputs['style']\n",
        "    content_outputs = outputs['content']\n",
        "    style_loss = tf.add_n([tf.reduce_mean((style_outputs[name]-style_targets[name])**2) \n",
        "                           for name in style_outputs.keys()])\n",
        "    style_loss *= style_weight / num_style_layers\n",
        "\n",
        "    content_loss = tf.add_n([tf.reduce_mean((content_outputs[name]-content_targets[name])**2) \n",
        "                             for name in content_outputs.keys()])\n",
        "    content_loss *= content_weight / num_content_layers\n",
        "    loss = style_loss + content_loss\n",
        "    return loss\n",
        "\n",
        "# FILL THE CODE: here you will do a custom training loop as explained in the slide\n",
        "# is a little bit different from the classic training loop since you will be operating\n",
        "# directly on the image, not on the model, what you need to do is:\n",
        "# -compute output\n",
        "# -compute loss\n",
        "# - compute gradient\n",
        "# - apply gradient\n",
        "# - clip the image: image.assign(clip_0_1(image))\n",
        "@tf.function()\n",
        "def train_step(image):\n",
        "   #....\n",
        "   #...\n",
        "   #...\n",
        "   #...\n",
        "   #...\n",
        "   #...\n",
        "   \n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-2mL2PCmZefW"
      },
      "source": [
        "\n",
        "# FILL THE CODE: perform a few training steps\n",
        "#...\n",
        "#...\n",
        "#...\n",
        "\n",
        "# Plot the image\n",
        "tensor_to_image(image)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "INHkWstdZtA4"
      },
      "source": [
        "Perform a longer optimization"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PtBOpCB2ZefX"
      },
      "source": [
        "import time\n",
        "start = time.time()\n",
        "\n",
        "epochs = 10\n",
        "steps_per_epoch = 10\n",
        "\n",
        "step = 0\n",
        "for n in range(epochs):\n",
        "    for m in range(steps_per_epoch):\n",
        "        step += 1\n",
        "        train_step(image)\n",
        "        print(\".\", end='')\n",
        "    display.clear_output(wait=True)\n",
        "    display.display(tensor_to_image(image))\n",
        "    print(\"Train step: {}\".format(step))\n",
        "end = time.time()\n",
        "print(\"Total time: {:.1f}\".format(end-start))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GdQqyGq7ZwqV"
      },
      "source": [
        "# Total Variation Loss\n",
        "\n",
        "\n",
        "*   This basic implementation produces a lot of high frequency artifacts\n",
        "*   We can decrease this using an explicit regularization term on the high-frequency components -> *total variation loss*\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zW40so2HZv2F"
      },
      "source": [
        "def high_pass_x_y(image):\n",
        "  x_var = image[:,:,1:,:] - image[:,:,:-1,:]\n",
        "  y_var = image[:,1:,:,:] - image[:,:-1,:,:]\n",
        "\n",
        "  return x_var, y_var"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NkdjsJjra4oN"
      },
      "source": [
        "Plot to show high-frequency components\n",
        "\n",
        "as you can see after style transfer, high-frequency components have increased"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SF9AU2ReZv7Y"
      },
      "source": [
        "\n",
        "x_deltas, y_deltas = high_pass_x_y(content_image)\n",
        "\n",
        "plt.figure(figsize=(14,10))\n",
        "plt.subplot(2,2,1)\n",
        "imshow(clip_0_1(2*y_deltas+0.5), \"Horizontal Deltas: Original\")\n",
        "\n",
        "plt.subplot(2,2,2)\n",
        "imshow(clip_0_1(2*x_deltas+0.5), \"Vertical Deltas: Original\")\n",
        "\n",
        "x_deltas, y_deltas = high_pass_x_y(image)\n",
        "\n",
        "plt.subplot(2,2,3)\n",
        "imshow(clip_0_1(2*y_deltas+0.5), \"Horizontal Deltas: Styled\")\n",
        "\n",
        "plt.subplot(2,2,4)\n",
        "imshow(clip_0_1(2*x_deltas+0.5), \"Vertical Deltas: Styled\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HaABuzA9bERs"
      },
      "source": [
        "The regularization loss associated with this is the sum of squares of the high-pass values (already implemented in tensorflow!)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "G37l5407bM1U"
      },
      "source": [
        "def total_variation_loss(image):\n",
        "  x_deltas, y_deltas = high_pass_x_y(image)\n",
        "  return tf.reduce_sum(tf.abs(x_deltas)) + tf.reduce_sum(tf.abs(y_deltas))\n",
        "\n",
        "# Custom\n",
        "total_variation_loss_custom =total_variation_loss(image).numpy()\n",
        "# Tensorflow\n",
        "total_variation_loss =tf.image.total_variation(image).numpy()\n",
        "\n",
        "print('Total variation loss (custom)',total_variation_loss_custom)\n",
        "print('Total variation loss',total_variation_loss)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "M284GE5EcDQp"
      },
      "source": [
        "# Re-run the optimization"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9rv5YqBhcFzm"
      },
      "source": [
        "# Choose a weight for the total_variation_loss\n",
        "total_variation_weight=30\n",
        "\n",
        "# FILL THE CODE: Include it in the train_step function\n",
        "# Repeat the same custom training loop (tf.GradientTape...) that you did before\n",
        "# only this take take into account the weighted total_variation loss and sum it\n",
        "# to the loss\n",
        "\n",
        "@tf.function()\n",
        "#...\n",
        "#...\n",
        "#...\n",
        "#...\n",
        "#..."
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "98v3iYt8caPb"
      },
      "source": [
        "# Re-initialize the optimization variable:\n",
        "image = tf.Variable(content_image)\n",
        "\n",
        "# Run the optimization\n",
        "import time\n",
        "start = time.time()\n",
        "\n",
        "epochs = 10\n",
        "steps_per_epoch = 100\n",
        "\n",
        "step = 0\n",
        "for n in range(epochs):\n",
        "  for m in range(steps_per_epoch):\n",
        "    step += 1\n",
        "    train_step(image)\n",
        "    print(\".\", end='')\n",
        "  display.clear_output(wait=True)\n",
        "  display.display(tensor_to_image(image))\n",
        "  print(\"Train step: {}\".format(step))\n",
        "\n",
        "end = time.time()\n",
        "print(\"Total time: {:.1f}\".format(end-start))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JT7DIlzZZee3"
      },
      "source": [
        "# Fast style transfer -> tensorflow hub"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mrQ0WJNVZee3"
      },
      "source": [
        "import tensorflow_hub as hub\n",
        "hub_module = hub.load('https://tfhub.dev/google/magenta/arbitrary-image-stylization-v1-256/1')\n",
        "stylized_image = hub_module(tf.constant(content_image), tf.constant(style_image))[0]\n",
        "tensor_to_image(stylized_image)"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}